CRUSH作为Ceph集群最为核心的数据分布算法，需要解决大规模分布式存储系统面临艰巨的任务。当前存储设备已经发展至数十、数百、甚至是数千的存储设备，而数据通常会达到PB这个数量级以至更高。当前的存储系统必须能够均匀的分配数据和工作负载，以高效的利用现有可用资源、系统性能， 同时要便于集群的扩展以及对硬件故障的处理。  
CRUSH算法正是实现了一个伪随机的数据分发功能，它被设计用于基于对象的分布式存储系统，这样的系统不依赖于某个中央目录就能够实现数据对象到存储设备的映射。另外大型存储系统的发展本质就是动态的，所以CRUSH被设计成便于增加和移除存储设备，同时能将非必要的数据移动降至最低。该算法还包括了各种各样的数据复制和可靠性机制，并按照用户定义的策略来分发数据，而且这样的策略还能强制执行跨越故障域的备份分离。

CRUSH Map  
CRUSH Map是用户为集群定义的一颗具有层次结构的树，如下图：

![](/assets/crush_3.png)

层次结构中包含只包含两种角色：

* Bucket（桶）：表示除 OSD 之外的角色
* Device（设备）：只有 OSD 才能是 Device 角色

在 CRUSH Map 中，叶子节点必须为 Device （即 OSD），非叶子节点必须为 Bucket。通常情况下，数据中心实际的物理拓扑来决定CRUSH Map，如上图示例，根节点下包含一个数据中心，该数据中心下有一个机房，该机房中有一个机架，该机架下有两台服务器，每台服务器下面有两个OSD。CRUSH Map 树结构能很好的来描述各个层级之间的包含关系，而 CRUSH 算法可以根据这个层级树以及定义好的 CRUSH RULE 来决定最终如何将 PG 落到 OSD 上。

当然 CRUSH MAP 某些时候还可以根据实际需求定义或划分为逻辑拓扑，比如：我们希望区分不同性能的存储设备

![](/assets/crush_2.png)

上图的拓扑可以定义如下CRUSH Map

```
$ sudo ceph osd tree
# id  weight  type name up/down reweight
-21 12  root ssd
-22 2       host ceph-osd2-ssd
6 1             osd.6 up  1
9 1             osd.9 up  1
-23 2       host ceph-osd1-ssd
8 1             osd.8 up  1
11  1           osd.11  up  1
-24 2       host ceph-osd0-ssd
7 1             osd.7 up  1
10  1           osd.10  up  1
-1  12  root sata
-2  2       host ceph-osd2-sata
0 1             osd.0 up  1
3 1             osd.3 up  1
-3  2       host ceph-osd1-sata
2 1             osd.2 up  1
5 1             osd.5 up  1
-4  2       host ceph-osd0-sata
1 1             osd.1 up  1
4 1             osd.4 up  1
```

总之，CRUSH Map 是一个层级视图，描述了各个 OSD 在层级中所处的角色与位置。



CRUSH Algorithm

CRUSH 算法描述了使用什么算法将 PG 映射至 OSD 的过程：

![](/assets/crush_4.png)



CRUSH RULES

CRUSH RULES 描述了

集群映射由设备和桶（buckets）组成层级树，设备和桶都有数值的描述和权重值。例如：

bucket可以包含任意多的设备或者其他的桶，并由此形成内部节点的存储层次结构；而设备总是在叶节点。存储设备的权重由管理员设置以控制每个OSD存储的数据量。大型存储系统往往会包含不同容量大小、不同性能的OSD，CRUSH算法可以根据设备的空间使用率来分布数据。  
桶可由任意可用存储的层次结构组成。例如，可以创建这样一个集群映射，用名为“shelf”的桶代表最低层的一个主机，用来包含主机上的磁盘设备，然后用名为“cabinet”的桶来包含在同一个机架上的主机。在一个大的系统中，代表机架的“cabinet”桶可能还会包含在“row”桶或者“room”桶里，数据通过一个伪随机类hash函数递归地分配到各个层级的桶元素中。

CRUSH算法根据种每个设备的权重尽可能概率平均地分配数据。分布算法是由集群可用存储资源以及其逻辑单元的map控制的。这个map的描述类似于一个大型存储场景的描述：存储服务器由一系列的机柜组成，机柜装满服务器，服务器装满磁盘。数据分配的策略是由定位规则来定义的，定位规则指定了集群中将保存多少个副本，以及数据副本的放置有什么限制。例如，可以指定数据有三个副本，这三个副本必须放置在不同列的机柜中，进而使得数据的三个副本不公用一条物理电路。  
给定一个输入x，CRUSH 算法将输出一个确定的有序的储存目标向量   ⃗R 。当输入x，CRUSH利用强大的多重整数hash函数根据集群map、定位规则、以及x计算出确定的、独立的、可靠的映射关系。CRUSH分配算法是伪随机算法，并且输入的内容和输出的储存位置之间是没有显性相关的，所以我们可以说CRUSH 算法在集群设备中生成了“伪集群”的数据副本。集群的设备对一个数据项目共享数据副本，对其他数据项目又是独立的。

